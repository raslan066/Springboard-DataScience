{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "import pylab\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# special matplotlib argument for improved plots\n",
    "from matplotlib import rcParams\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"Encoded_last.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(['CANCELLED'],axis=1)\n",
    "y=df['CANCELLED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer()\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the size to 10%\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, X, _, y = train_test_split(X,y,test_size=0.1, random_state=30, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(ratio='minority')\n",
    "X_sm, y_sm = smote.fit_sample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_sm[y_sm==1]))\n",
    "print(len(y_sm[y_sm==0]))\n",
    "\n",
    "print(len(y[y==1]))\n",
    "print(len(y[y==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sm,y_sm,test_size=0.3, random_state=30, stratify=y_sm)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=30, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline as pl\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# Steps for pipeline\n",
    "steps = [(\"scaler\", MinMaxScaler(feature_range=(0,1))),\n",
    "         (\"model\", LogisticRegression(random_state=40))]\n",
    "pipeline = pl(steps)\n",
    "\n",
    "# Parameters\n",
    "param_grid = dict(model__C = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "                  model__penalty = ['l1', 'l2'],\n",
    "                  model__class_weight = ['balanced'])\n",
    "\n",
    "# Setting up the grid search\n",
    "LRcw = GridSearchCV(pipeline, param_grid = param_grid, \n",
    "                           verbose = 3,\n",
    "                           cv = 5,\n",
    "                          n_jobs=-1,\n",
    "                          scoring='average_precision',\n",
    "                          iid=False)\n",
    "# Training using CV\n",
    "LRcw.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, cohen_kappa_score\n",
    "from sklearn import metrics\n",
    "print(\"best params is : \",LRcw.best_params_)\n",
    "print(\"best score is : \", LRcw.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, cohen_kappa_score\n",
    "from sklearn import metrics\n",
    "print(\"best params is : \",LRcw.best_params_)\n",
    "print(\"best score is : \", LRcw.best_score_)\n",
    "y_pred_test = LRcw.predict(X_test)\n",
    "y_pred_test_prob = LRcw.predict_proba(X_test)\n",
    "print(\"#####################\")\n",
    "print(\"Test data\")\n",
    "print(\"#####################\")\n",
    "print(\"F1: \", metrics.f1_score(y_test, y_pred_test))\n",
    "print(\"Cohen Kappa: \", metrics.cohen_kappa_score(y_test, y_pred_test))\n",
    "print(\"Brier: \", metrics.brier_score_loss(y_test, y_pred_test))\n",
    "print(\"LogLoss: \", metrics.log_loss(y_test, y_pred_test_prob))\n",
    "print(metrics.classification_report(y_test, y_pred_test))\n",
    "conf_matrix = metrics.confusion_matrix(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.matshow(conf_matrix, alpha=0.3,cmap=plt.cm.gray_r)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf_matrix[i, j], va='center', ha='center', fontsize=24,color ='k')\n",
    "plt.title('Confusion Matrix ', size=20)\n",
    "plt.xlabel('Predicted label', size=20)\n",
    "plt.ylabel('True label', size=20)\n",
    "ax.tick_params('x', labelsize = 20)\n",
    "ax.tick_params('y', labelsize = 20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_test_prob[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, lw=1, label='Model Performance (area = %0.2f)' % (roc_auc))\n",
    "plt.plot([0, 1], \n",
    "            [0, 1], \n",
    "             '--', \n",
    "             color=(0.6, 0.6, 0.6), \n",
    "             label='Luck')\n",
    "    \n",
    "plt.plot([0, 0, 1], \n",
    "             [0, 1, 1], \n",
    "             lw=2,\n",
    "             linestyle=':',\n",
    "             color='black',\n",
    "             label='Perfect Performance')\n",
    "        \n",
    "plt.xlim([-0.02, 1.02])\n",
    "plt.ylim([-0.02, 1.02])\n",
    "plt.xlabel('False Positive Rate', size=20)\n",
    "plt.ylabel('True Positive Rate', size=20)\n",
    "plt.title('Receiver Operating Characteristic', size=20)\n",
    "plt.xticks(size = 20)\n",
    "plt.yticks(size = 20)\n",
    "plt.legend(loc=\"lower right\", fontsize=18)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR curve\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 8))\n",
    "prec, recall, thresholds = precision_recall_curve(y_test, y_pred_test_prob[:, 1])\n",
    "#pr_auc = auc(prec, recall)\n",
    "plt.plot(recall, prec, lw=1, label='Model Performance (area = %0.2f)' % \n",
    "             (metrics.average_precision_score(y_test, y_pred_test_prob[:, 1])))    \n",
    "plt.xlim([-0.02, 1.02])\n",
    "plt.ylim([-0.02, 1.02])\n",
    "plt.xlabel('Recall', size=20)\n",
    "plt.ylabel('Precision', size=20)\n",
    "plt.title('PR Curve for', size=20)\n",
    "plt.xticks(size = 20)\n",
    "plt.yticks(size = 20)\n",
    "plt.legend(loc=\"top right\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
